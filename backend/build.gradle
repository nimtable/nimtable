/*
 * Copyright 2025 Nimtable
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

plugins {
  id 'java'
  id 'application'
  id 'com.github.johnrengelman.shadow' version '8.1.1'
  id 'com.diffplug.spotless' version '6.25.0'
}

java {
  toolchain {
    languageVersion = JavaLanguageVersion.of(17)
  }
  sourceCompatibility = JavaVersion.VERSION_17
  targetCompatibility = JavaVersion.VERSION_17
}

repositories {
  mavenCentral()
}

application {
  mainClass = 'io.nimtable.Server'
  applicationDefaultJvmArgs = [
    // Required by Spark when using Java 17+
    // See https://github.com/apache/spark/blob/v3.3.0/launcher/src/main/java/org/apache/spark/launcher/JavaModuleOptions.java
    "--add-opens=java.base/java.lang=ALL-UNNAMED",
    "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED",
    "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED",
    "--add-opens=java.base/java.io=ALL-UNNAMED",
    "--add-opens=java.base/java.net=ALL-UNNAMED",
    "--add-opens=java.base/java.nio=ALL-UNNAMED",
    "--add-opens=java.base/java.util=ALL-UNNAMED",
    "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED",
    "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED",
    "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED",
    "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED",
    "--add-opens=java.base/sun.security.action=ALL-UNNAMED",
    "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED",
    "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED"
  ]
}

ext {
  icebergVersion = '1.8.1'
  hadoopVersion = '3.3.6'
  jettyVersion = '11.0.15'
  sparkVersion = '3.5.1'
  scalaVersion = '2.12.18'
}

dependencies {
  implementation "org.apache.iceberg:iceberg-api:${icebergVersion}"
  implementation "org.apache.iceberg:iceberg-core:${icebergVersion}"
  implementation "org.apache.iceberg:iceberg-aws:${icebergVersion}"
  implementation "org.apache.iceberg:iceberg-azure:${icebergVersion}"
  implementation "org.apache.iceberg:iceberg-gcp:${icebergVersion}"
  implementation "org.apache.iceberg:iceberg-bundled-guava:${icebergVersion}"

  implementation "org.apache.hadoop:hadoop-common:${hadoopVersion}"
  implementation "org.apache.hadoop:hadoop-hdfs-client:${hadoopVersion}"

  runtimeOnly "org.apache.iceberg:iceberg-aws-bundle:${icebergVersion}"
  runtimeOnly "org.apache.iceberg:iceberg-azure-bundle:${icebergVersion}"
  runtimeOnly "org.apache.iceberg:iceberg-gcp-bundle:${icebergVersion}"

  implementation 'org.apache.httpcomponents.client5:httpclient5:5.3.1'
  implementation 'org.apache.httpcomponents.core5:httpcore5:5.2.5'

  implementation 'com.google.guava:guava:33.3.0-jre'

  implementation 'org.xerial:sqlite-jdbc:3.46.1.0'
  implementation 'org.postgresql:postgresql:42.7.4'

  // DB dependencies
  // flyway for migrations
  implementation 'org.flywaydb:flyway-core:10.15.2'
  // HikariCP for connection pooling
  implementation 'com.zaxxer:HikariCP:5.1.0'
  // Password Hashing
  implementation 'org.mindrot:jbcrypt:0.4'

  implementation 'com.fasterxml.jackson.core:jackson-databind:2.18.2'
  implementation 'com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:2.18.2'

  implementation "org.eclipse.jetty:jetty-server:${jettyVersion}"
  implementation "org.eclipse.jetty:jetty-servlet:${jettyVersion}"
  implementation "org.eclipse.jetty:jetty-util:${jettyVersion}"
  implementation "org.eclipse.jetty:jetty-util-ajax:${jettyVersion}"

  implementation 'org.slf4j:slf4j-api:2.0.12'
  implementation 'org.slf4j:slf4j-log4j12:2.0.12'
  implementation 'log4j:log4j:1.2.17'

  // Spark dependencies
  implementation "org.apache.spark:spark-core_2.12:${sparkVersion}"
  implementation "org.apache.spark:spark-sql_2.12:${sparkVersion}"
  implementation "org.apache.spark:spark-hive_2.12:${sparkVersion}"
  implementation "org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:${icebergVersion}"
  implementation 'org.scala-lang.modules:scala-collection-compat_2.12:2.12.0'
}

jar {
  manifest {
    attributes 'Main-Class': 'io.nimtable.Server'
  }
}

shadowJar {
  mergeServiceFiles()
  zip64 = true
}

build.dependsOn(shadowJar)

task buildFrontend(type: Exec) {
  workingDir '../'
  commandLine 'sh', '-c', 'npm install && npm run build'
}

task copyFrontend(type: Copy, dependsOn: buildFrontend) {
  from '../out'
  into 'src/main/resources/static'
  duplicatesStrategy = DuplicatesStrategy.INCLUDE
}

processResources {
  dependsOn copyFrontend
  from('src/main/resources') {
    include '**/*'
  }
  duplicatesStrategy = DuplicatesStrategy.INCLUDE
}

// Ensure run task has access to resources
run {
  dependsOn processResources
  classpath += sourceSets.main.runtimeClasspath
}

// Clean up copied frontend files
clean {
  delete 'src/main/resources/static'
}

// Task to download all dependencies
task downloadDependencies {
  description = 'Downloads all project dependencies for offline development and cache warming.'
  group = 'build setup'
  
  doLast {
    def allDeps = configurations.compileClasspath.resolve() + configurations.runtimeClasspath.resolve()
    println "Dependencies resolved and downloaded to Gradle cache"
    println "Total dependencies: ${allDeps.size()}"
    allDeps.each { println " - ${it.name}" }
  }
}

// Spotless configuration
spotless {
  java {
    googleJavaFormat("1.22.0").aosp()
    importOrder()
    removeUnusedImports()
    trimTrailingWhitespace()
    endWithNewline()
  }
}

// Task to analyze JAR sizes to help identify large dependencies
task analyzeDependencies {
  description = 'Analyzes the size of all dependencies to help identify large ones'
  group = 'build setup'
  
  doLast {
    def resolvedDeps = configurations.runtimeClasspath.resolvedConfiguration.resolvedArtifacts
    def sortedDeps = resolvedDeps.sort { a, b -> b.file.length() <=> a.file.length() }
    println "Dependencies sorted by size:"
    sortedDeps.take(20).each { artifact ->
      def sizeMB = artifact.file.length() / (1024 * 1024)
      println String.format("%-70s %8.2f MB", "${artifact.moduleVersion.id.group}:${artifact.moduleVersion.id.name}:${artifact.moduleVersion.id.version}", sizeMB)
    }
  }
}
